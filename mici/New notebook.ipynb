{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "datalore": {
     "sheet_delimiter": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.6.* in /opt/python/envs/default/lib/python3.8/site-packages (2.6.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras==2.6.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "datalore": {
     "sheet_delimiter": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 15:57:19.862444: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-20 15:57:19.862467: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(1)\n",
    "from tensorflow.python.keras.layers import Dense, Dropout\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "datalore": {
     "sheet_delimiter": false
    }
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('train.csv')\n",
    "X = df.drop(columns = 'critical_temp')\n",
    "y = df['critical_temp']\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_elements</th>\n",
       "      <th>mean_atomic_mass</th>\n",
       "      <th>wtd_mean_atomic_mass</th>\n",
       "      <th>gmean_atomic_mass</th>\n",
       "      <th>wtd_gmean_atomic_mass</th>\n",
       "      <th>entropy_atomic_mass</th>\n",
       "      <th>wtd_entropy_atomic_mass</th>\n",
       "      <th>range_atomic_mass</th>\n",
       "      <th>wtd_range_atomic_mass</th>\n",
       "      <th>std_atomic_mass</th>\n",
       "      <th>...</th>\n",
       "      <th>wtd_mean_Valence</th>\n",
       "      <th>gmean_Valence</th>\n",
       "      <th>wtd_gmean_Valence</th>\n",
       "      <th>entropy_Valence</th>\n",
       "      <th>wtd_entropy_Valence</th>\n",
       "      <th>range_Valence</th>\n",
       "      <th>wtd_range_Valence</th>\n",
       "      <th>std_Valence</th>\n",
       "      <th>wtd_std_Valence</th>\n",
       "      <th>critical_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.862692</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.116612</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.062396</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>31.794921</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.219783</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.066221</td>\n",
       "      <td>1</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.437059</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>92.729214</td>\n",
       "      <td>58.518416</td>\n",
       "      <td>73.132787</td>\n",
       "      <td>36.396602</td>\n",
       "      <td>1.449309</td>\n",
       "      <td>1.057755</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>36.161939</td>\n",
       "      <td>47.094633</td>\n",
       "      <td>...</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>1.888175</td>\n",
       "      <td>2.210679</td>\n",
       "      <td>1.557113</td>\n",
       "      <td>1.047221</td>\n",
       "      <td>2</td>\n",
       "      <td>1.128571</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.468606</td>\n",
       "      <td>26.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.885242</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.122509</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>0.975980</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>35.741099</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.271429</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.232679</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.029175</td>\n",
       "      <td>1</td>\n",
       "      <td>1.114286</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.444697</td>\n",
       "      <td>19.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.873967</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.119560</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.022291</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>33.768010</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.264286</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.226222</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.048834</td>\n",
       "      <td>1</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.440952</td>\n",
       "      <td>22.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.840143</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.110716</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.129224</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>27.848743</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.242857</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.206963</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.096052</td>\n",
       "      <td>1</td>\n",
       "      <td>1.057143</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.428809</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21258</th>\n",
       "      <td>4</td>\n",
       "      <td>106.957877</td>\n",
       "      <td>53.095769</td>\n",
       "      <td>82.515384</td>\n",
       "      <td>43.135565</td>\n",
       "      <td>1.177145</td>\n",
       "      <td>1.254119</td>\n",
       "      <td>146.88130</td>\n",
       "      <td>15.504479</td>\n",
       "      <td>65.764081</td>\n",
       "      <td>...</td>\n",
       "      <td>3.555556</td>\n",
       "      <td>3.223710</td>\n",
       "      <td>3.519911</td>\n",
       "      <td>1.377820</td>\n",
       "      <td>0.913658</td>\n",
       "      <td>1</td>\n",
       "      <td>2.168889</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.496904</td>\n",
       "      <td>2.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21259</th>\n",
       "      <td>5</td>\n",
       "      <td>92.266740</td>\n",
       "      <td>49.021367</td>\n",
       "      <td>64.812662</td>\n",
       "      <td>32.867748</td>\n",
       "      <td>1.323287</td>\n",
       "      <td>1.571630</td>\n",
       "      <td>188.38390</td>\n",
       "      <td>7.353333</td>\n",
       "      <td>69.232655</td>\n",
       "      <td>...</td>\n",
       "      <td>2.047619</td>\n",
       "      <td>2.168944</td>\n",
       "      <td>2.038991</td>\n",
       "      <td>1.594167</td>\n",
       "      <td>1.337246</td>\n",
       "      <td>1</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.212959</td>\n",
       "      <td>122.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21260</th>\n",
       "      <td>2</td>\n",
       "      <td>99.663190</td>\n",
       "      <td>95.609104</td>\n",
       "      <td>99.433882</td>\n",
       "      <td>95.464320</td>\n",
       "      <td>0.690847</td>\n",
       "      <td>0.530198</td>\n",
       "      <td>13.51362</td>\n",
       "      <td>53.041104</td>\n",
       "      <td>6.756810</td>\n",
       "      <td>...</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>4.472136</td>\n",
       "      <td>4.781762</td>\n",
       "      <td>0.686962</td>\n",
       "      <td>0.450561</td>\n",
       "      <td>1</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21261</th>\n",
       "      <td>2</td>\n",
       "      <td>99.663190</td>\n",
       "      <td>97.095602</td>\n",
       "      <td>99.433882</td>\n",
       "      <td>96.901083</td>\n",
       "      <td>0.690847</td>\n",
       "      <td>0.640883</td>\n",
       "      <td>13.51362</td>\n",
       "      <td>31.115202</td>\n",
       "      <td>6.756810</td>\n",
       "      <td>...</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>4.472136</td>\n",
       "      <td>4.665819</td>\n",
       "      <td>0.686962</td>\n",
       "      <td>0.577601</td>\n",
       "      <td>1</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.462493</td>\n",
       "      <td>1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21262</th>\n",
       "      <td>3</td>\n",
       "      <td>87.468333</td>\n",
       "      <td>86.858500</td>\n",
       "      <td>82.555758</td>\n",
       "      <td>80.458722</td>\n",
       "      <td>1.041270</td>\n",
       "      <td>0.895229</td>\n",
       "      <td>71.75500</td>\n",
       "      <td>43.144000</td>\n",
       "      <td>29.905282</td>\n",
       "      <td>...</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.762203</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>1.054920</td>\n",
       "      <td>0.970116</td>\n",
       "      <td>3</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21263 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n",
       "0                       4         88.944468             57.862692   \n",
       "1                       5         92.729214             58.518416   \n",
       "2                       4         88.944468             57.885242   \n",
       "3                       4         88.944468             57.873967   \n",
       "4                       4         88.944468             57.840143   \n",
       "...                   ...               ...                   ...   \n",
       "21258                   4        106.957877             53.095769   \n",
       "21259                   5         92.266740             49.021367   \n",
       "21260                   2         99.663190             95.609104   \n",
       "21261                   2         99.663190             97.095602   \n",
       "21262                   3         87.468333             86.858500   \n",
       "\n",
       "       gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n",
       "0              66.361592              36.116612             1.181795   \n",
       "1              73.132787              36.396602             1.449309   \n",
       "2              66.361592              36.122509             1.181795   \n",
       "3              66.361592              36.119560             1.181795   \n",
       "4              66.361592              36.110716             1.181795   \n",
       "...                  ...                    ...                  ...   \n",
       "21258          82.515384              43.135565             1.177145   \n",
       "21259          64.812662              32.867748             1.323287   \n",
       "21260          99.433882              95.464320             0.690847   \n",
       "21261          99.433882              96.901083             0.690847   \n",
       "21262          82.555758              80.458722             1.041270   \n",
       "\n",
       "       wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n",
       "0                     1.062396          122.90607              31.794921   \n",
       "1                     1.057755          122.90607              36.161939   \n",
       "2                     0.975980          122.90607              35.741099   \n",
       "3                     1.022291          122.90607              33.768010   \n",
       "4                     1.129224          122.90607              27.848743   \n",
       "...                        ...                ...                    ...   \n",
       "21258                 1.254119          146.88130              15.504479   \n",
       "21259                 1.571630          188.38390               7.353333   \n",
       "21260                 0.530198           13.51362              53.041104   \n",
       "21261                 0.640883           13.51362              31.115202   \n",
       "21262                 0.895229           71.75500              43.144000   \n",
       "\n",
       "       std_atomic_mass  ...  wtd_mean_Valence  gmean_Valence  \\\n",
       "0            51.968828  ...          2.257143       2.213364   \n",
       "1            47.094633  ...          2.257143       1.888175   \n",
       "2            51.968828  ...          2.271429       2.213364   \n",
       "3            51.968828  ...          2.264286       2.213364   \n",
       "4            51.968828  ...          2.242857       2.213364   \n",
       "...                ...  ...               ...            ...   \n",
       "21258        65.764081  ...          3.555556       3.223710   \n",
       "21259        69.232655  ...          2.047619       2.168944   \n",
       "21260         6.756810  ...          4.800000       4.472136   \n",
       "21261         6.756810  ...          4.690000       4.472136   \n",
       "21262        29.905282  ...          4.500000       4.762203   \n",
       "\n",
       "       wtd_gmean_Valence  entropy_Valence  wtd_entropy_Valence  range_Valence  \\\n",
       "0               2.219783         1.368922             1.066221              1   \n",
       "1               2.210679         1.557113             1.047221              2   \n",
       "2               2.232679         1.368922             1.029175              1   \n",
       "3               2.226222         1.368922             1.048834              1   \n",
       "4               2.206963         1.368922             1.096052              1   \n",
       "...                  ...              ...                  ...            ...   \n",
       "21258           3.519911         1.377820             0.913658              1   \n",
       "21259           2.038991         1.594167             1.337246              1   \n",
       "21260           4.781762         0.686962             0.450561              1   \n",
       "21261           4.665819         0.686962             0.577601              1   \n",
       "21262           4.242641         1.054920             0.970116              3   \n",
       "\n",
       "       wtd_range_Valence  std_Valence  wtd_std_Valence  critical_temp  \n",
       "0               1.085714     0.433013         0.437059          29.00  \n",
       "1               1.128571     0.632456         0.468606          26.00  \n",
       "2               1.114286     0.433013         0.444697          19.00  \n",
       "3               1.100000     0.433013         0.440952          22.00  \n",
       "4               1.057143     0.433013         0.428809          23.00  \n",
       "...                  ...          ...              ...            ...  \n",
       "21258           2.168889     0.433013         0.496904           2.44  \n",
       "21259           0.904762     0.400000         0.212959         122.10  \n",
       "21260           3.200000     0.500000         0.400000           1.98  \n",
       "21261           2.210000     0.500000         0.462493           1.84  \n",
       "21262           1.800000     1.414214         1.500000          12.80  \n",
       "\n",
       "[21263 rows x 82 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21263, 82)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "datalore": {
     "sheet_delimiter": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38958/1746069364.py:3: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n"
     ]
    }
   ],
   "source": [
    "#Filter the data\n",
    "cor_matrix = df.corr().abs()\n",
    "upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]#IF THE CORRELATION IS HIGHER THAN 0.9 WE CAN DROP THE COLUMNS\n",
    "df_copy = df.copy(deep=True)\n",
    "df_copy.drop(columns=to_drop,inplace=True)\n",
    "X = df_copy.drop(columns = ['critical_temp'])\n",
    "y = df.critical_temp\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "sheet_delimiter": false
    }
   },
   "source": [
    "## Scaling data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "datalore": {
     "sheet_delimiter": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n",
      "MinMaxScaler()\n"
     ]
    }
   ],
   "source": [
    "y_train=np.reshape(y_train, (-1,1))\n",
    "y_test=np.reshape(y_test, (-1,1))\n",
    "scaler_x = MinMaxScaler()\n",
    "print(scaler_x.fit(X_train))\n",
    "xtrain_scale=scaler_x.transform(X_train)\n",
    "print(scaler_x.fit(X_test))\n",
    "xval_scale=scaler_x.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "datalore": {
     "sheet_delimiter": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15947, 43)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_scale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "datalore": {
     "sheet_delimiter": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 43)                1892      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3000)              132000    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2500)              7502500   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2000)              5002000   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1500)              3001500   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              1501000   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 300)               150300    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 76        \n",
      "=================================================================\n",
      "Total params: 17,848,243\n",
      "Trainable params: 17,848,243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 15:59:06.334652: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-20 15:59:06.334689: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-20 15:59:06.334722: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mobile): /proc/driver/nvidia/version does not exist\n",
      "2021-12-20 15:59:06.335503: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(43, input_dim = 43, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(3000,activation='relu',kernel_initializer='normal'))\n",
    "model.add(Dense(2500,activation='relu',kernel_initializer='normal'))\n",
    "model.add(Dense(2000,activation='relu', kernel_initializer='normal'))\n",
    "model.add(Dense(1500,activation='relu',kernel_initializer='normal'))\n",
    "model.add(Dense(1000,activation='relu',kernel_initializer='normal'))\n",
    "model.add(Dense(500,activation='relu', kernel_initializer='normal'))\n",
    "model.add(Dense(300,activation='relu',kernel_initializer='normal'))\n",
    "model.add(Dense(150,activation='relu',kernel_initializer='normal'))\n",
    "model.add(Dense(75,activation='relu', kernel_initializer='normal'))\n",
    "model.add(Dense(1,kernel_initializer='normal'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "datalore": {
     "sheet_delimiter": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "128/128 [==============================] - 11s 85ms/step - loss: 315.4663 - root_mean_squared_error: 20.6856 - val_loss: 211.9978 - val_root_mean_squared_error: 17.4495\n",
      "Epoch 2/30\n",
      "128/128 [==============================] - 11s 83ms/step - loss: 244.2948 - root_mean_squared_error: 16.7123 - val_loss: 206.9920 - val_root_mean_squared_error: 16.4046\n",
      "Epoch 3/30\n",
      "128/128 [==============================] - 11s 84ms/step - loss: 230.2813 - root_mean_squared_error: 16.1392 - val_loss: 217.4611 - val_root_mean_squared_error: 15.9559\n",
      "Epoch 4/30\n",
      "128/128 [==============================] - 11s 83ms/step - loss: 237.6722 - root_mean_squared_error: 15.8448 - val_loss: 264.1480 - val_root_mean_squared_error: 15.8205\n",
      "Epoch 5/30\n",
      "128/128 [==============================] - 11s 84ms/step - loss: 245.7680 - root_mean_squared_error: 15.8293 - val_loss: 198.2969 - val_root_mean_squared_error: 15.7734\n",
      "Epoch 6/30\n",
      "128/128 [==============================] - 11s 84ms/step - loss: 225.4070 - root_mean_squared_error: 15.6968 - val_loss: 191.0741 - val_root_mean_squared_error: 15.6126\n",
      "Epoch 7/30\n",
      "128/128 [==============================] - 11s 84ms/step - loss: 216.8402 - root_mean_squared_error: 15.5202 - val_loss: 206.6596 - val_root_mean_squared_error: 15.4722\n",
      "Epoch 8/30\n",
      "128/128 [==============================] - 11s 84ms/step - loss: 246.8216 - root_mean_squared_error: 15.4708 - val_loss: 230.7091 - val_root_mean_squared_error: 15.4804\n",
      "Epoch 9/30\n",
      "128/128 [==============================] - 11s 87ms/step - loss: 221.5031 - root_mean_squared_error: 15.4375 - val_loss: 312.7549 - val_root_mean_squared_error: 15.4496\n",
      "Epoch 10/30\n",
      "128/128 [==============================] - 11s 89ms/step - loss: 230.5358 - root_mean_squared_error: 15.4750 - val_loss: 189.0335 - val_root_mean_squared_error: 15.4362\n",
      "Epoch 11/30\n",
      "128/128 [==============================] - 11s 87ms/step - loss: 210.6688 - root_mean_squared_error: 15.3865 - val_loss: 186.1688 - val_root_mean_squared_error: 15.3398\n",
      "Epoch 12/30\n",
      "128/128 [==============================] - 11s 88ms/step - loss: 211.6455 - root_mean_squared_error: 15.2984 - val_loss: 202.8011 - val_root_mean_squared_error: 15.2667\n",
      "Epoch 13/30\n",
      "128/128 [==============================] - 11s 83ms/step - loss: 202.7289 - root_mean_squared_error: 15.2299 - val_loss: 185.8952 - val_root_mean_squared_error: 15.1837\n",
      "Epoch 14/30\n",
      "128/128 [==============================] - 11s 83ms/step - loss: 212.1192 - root_mean_squared_error: 15.1472 - val_loss: 223.1152 - val_root_mean_squared_error: 15.1371\n",
      "Epoch 15/30\n",
      "128/128 [==============================] - 11s 84ms/step - loss: 200.5727 - root_mean_squared_error: 15.1093 - val_loss: 249.2873 - val_root_mean_squared_error: 15.0910\n",
      "Epoch 16/30\n",
      "128/128 [==============================] - 11s 84ms/step - loss: 216.2068 - root_mean_squared_error: 15.0919 - val_loss: 245.3535 - val_root_mean_squared_error: 15.0780\n",
      "Epoch 17/30\n",
      "128/128 [==============================] - 11s 83ms/step - loss: 197.5926 - root_mean_squared_error: 15.0574 - val_loss: 201.8902 - val_root_mean_squared_error: 15.0302\n",
      "Epoch 18/30\n",
      "128/128 [==============================] - 11s 84ms/step - loss: 201.4765 - root_mean_squared_error: 15.0017 - val_loss: 229.3008 - val_root_mean_squared_error: 14.9893\n",
      "Epoch 19/30\n",
      "128/128 [==============================] - 11s 84ms/step - loss: 191.8765 - root_mean_squared_error: 14.9630 - val_loss: 195.1631 - val_root_mean_squared_error: 14.9392\n",
      "Epoch 20/30\n",
      "128/128 [==============================] - 11s 84ms/step - loss: 196.9382 - root_mean_squared_error: 14.9128 - val_loss: 181.0841 - val_root_mean_squared_error: 14.8915\n",
      "Epoch 21/30\n",
      "128/128 [==============================] - 11s 83ms/step - loss: 196.6400 - root_mean_squared_error: 14.8719 - val_loss: 200.5513 - val_root_mean_squared_error: 14.8491\n",
      "Epoch 22/30\n",
      "128/128 [==============================] - 11s 83ms/step - loss: 204.5744 - root_mean_squared_error: 14.8334 - val_loss: 203.1445 - val_root_mean_squared_error: 14.8248\n",
      "Epoch 23/30\n",
      "128/128 [==============================] - 11s 84ms/step - loss: 196.1571 - root_mean_squared_error: 14.8096 - val_loss: 207.3078 - val_root_mean_squared_error: 14.7916\n",
      "Epoch 24/30\n",
      "128/128 [==============================] - 11s 86ms/step - loss: 177.2250 - root_mean_squared_error: 14.7668 - val_loss: 167.4311 - val_root_mean_squared_error: 14.7363\n",
      "Epoch 25/30\n",
      "128/128 [==============================] - 11s 87ms/step - loss: 178.9888 - root_mean_squared_error: 14.7059 - val_loss: 172.6301 - val_root_mean_squared_error: 14.6812\n",
      "Epoch 26/30\n",
      "128/128 [==============================] - 11s 89ms/step - loss: 177.8538 - root_mean_squared_error: 14.6552 - val_loss: 167.4028 - val_root_mean_squared_error: 14.6299\n",
      "Epoch 27/30\n",
      "128/128 [==============================] - 11s 89ms/step - loss: 173.7038 - root_mean_squared_error: 14.5981 - val_loss: 184.2734 - val_root_mean_squared_error: 14.5791\n",
      "Epoch 28/30\n",
      "128/128 [==============================] - 11s 88ms/step - loss: 176.8732 - root_mean_squared_error: 14.5553 - val_loss: 166.1087 - val_root_mean_squared_error: 14.5349\n",
      "Epoch 29/30\n",
      "128/128 [==============================] - 11s 86ms/step - loss: 176.8830 - root_mean_squared_error: 14.5128 - val_loss: 162.5173 - val_root_mean_squared_error: 14.4908\n",
      "Epoch 30/30\n",
      "128/128 [==============================] - 11s 85ms/step - loss: 173.8443 - root_mean_squared_error: 14.4678 - val_loss: 179.2268 - val_root_mean_squared_error: 14.4485\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=[tensorflow.keras.metrics.RootMeanSquaredError()])\n",
    "training=model.fit(xtrain_scale, y_train, epochs=30, batch_size=100, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "datalore": {
     "sheet_delimiter": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
