{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1eba980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 03:57:48.198807: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-22 03:57:48.198831: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, activations, metrics, optimizers, losses, utils, preprocessing\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de566c72",
   "metadata": {},
   "source": [
    "The first big question: What library to use to read in images. Some options are: \n",
    "\n",
    "1. Either use matplotlibs imread, directly loads an image as numpy array and installed everywhere\n",
    "2. Use pythons standard PIL (Pillow) library\n",
    "3. Use tf.keras.preprocessing.image.ImageDataGenerator\n",
    "4. Use skimage \n",
    "\n",
    "Each lib has its pros and cons. In the end the main focus here will be a CNN, so TF functionality will be important. \n",
    "Even for the other ML algorithms it may be wise to use the Convolutional Layers of the CNN to reduce the dimensionality of the data. (Note: TF internally uses Pillow.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2c28d4",
   "metadata": {},
   "source": [
    "First, lets get familiar with the dataset here, look at the dimensions of the images (without reading them in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f08739a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[72, 72]]), array([57720]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOURCE_DIR = \"photoz_images/\"\n",
    "imgs = [PIL.Image.open(SOURCE_DIR+filename).size for filename in os.listdir(SOURCE_DIR)]\n",
    "\n",
    "# see how many images of which size we have\n",
    "sizes, counts = np.unique(imgs, axis=0, return_counts=True)\n",
    "sizes, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e63a5c6",
   "metadata": {},
   "source": [
    "As all images are of the same size, we won't need heavy preprosessing, as we can use the original size, since 72x72 is quite handable. (Let's hope I don't regret saying that later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf0dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE = re.compile(r\"^(?P<ID>\\d+)_z_(?P<redshift>\\d+\\.\\d*(e-\\d+)?).jpg$\")\n",
    "def load_image(source_dir, filename):\n",
    "    img = img_to_array(load_img(source_dir+filename), dtype=np.float16)/255\n",
    "    matches = RE.match(filename)\n",
    "    return img, float(matches.group(\"redshift\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b88261c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3d74bbf76c4b2bbbf82fecd6ecaca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dirlist = list(os.listdir(SOURCE_DIR))\n",
    "images = np.empty((len(dirlist), 72, 72, 3), np.float16)\n",
    "redshifts = np.empty((len(dirlist),), np.float32)\n",
    "for idx, filename in enumerate(tqdm(dirlist)):\n",
    "    image, redshift = load_image(SOURCE_DIR, filename)\n",
    "    images[idx] = image\n",
    "    redshifts[idx] = redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8dd7bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, redshifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fbdb2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-22 03:58:18.966823: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-22 03:58:18.966862: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-22 03:58:18.966895: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mobile): /proc/driver/nvidia/version does not exist\n",
      "2021-12-22 03:58:18.968719: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# test CNN\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation=activations.swish, input_shape=(72, 72, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation=activations.swish))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(16, (3, 3), activation=activations.swish))\n",
    "model.add(layers.MaxPooling2D((3, 3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(60, activation=activations.swish))\n",
    "model.add(layers.Dropout(0.8))\n",
    "model.add(layers.Dense(60, activation=activations.swish))\n",
    "model.add(layers.Dense(1, activation=activations.linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b2bea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=losses.MeanSquaredError(),\n",
    "              metrics=[metrics.MeanAbsoluteError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb2d088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 70, 70, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 35, 35, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 33, 33, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 16)        4624      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                15420     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 60)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 60)                3660      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,909\n",
      "Trainable params: 33,909\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c44628c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.NumpyArrayIterator at 0x7efcf64bad30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = preprocessing.image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3d1405a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1353/1353 [==============================] - 90s 66ms/step - loss: 0.0151 - mean_absolute_error: 0.0854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc241f8e50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(generator.flow(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "195d4eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.36915752],\n",
       "        [0.3828466 ],\n",
       "        [0.05629794],\n",
       "        [0.12514882],\n",
       "        [0.18127477]], dtype=float32),\n",
       " array([0.323337 , 0.619709 , 0.0361306, 0.076164 , 0.0831686],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[:5]), y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187d3bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
